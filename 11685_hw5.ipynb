{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torchsummaryX\n",
        "!pip install -q cohere tiktoken openai==0.28\n",
        "!pip install -q datasets\n",
        "!pip install -q wandb"
      ],
      "metadata": {
        "id": "iXBe_R9F27kt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82d77413-8aff-4c50-f3d1-503789da3c1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchsummaryX in /usr/local/lib/python3.10/dist-packages (1.3.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchsummaryX) (2.1.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchsummaryX) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from torchsummaryX) (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->torchsummaryX) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->torchsummaryX) (2023.3.post1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchsummaryX) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchsummaryX) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchsummaryX) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchsummaryX) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchsummaryX) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchsummaryX) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchsummaryX) (2.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->torchsummaryX) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchsummaryX) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchsummaryX) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate -q\n",
        "!pip install rouge_score -q\n",
        "!pip install nltk -q\n",
        "!pip install absl -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYF-Wo3JLRad",
        "outputId": "3a676241-1ed8-41da-f7b9-bfeb4fc8630a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement absl (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for absl\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_-_Rmz_0-pE"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "from torchsummaryX import summary\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import tiktoken\n",
        "from datasets import load_dataset\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import os\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN = True\n",
        "\n",
        "# block_size = 100\n",
        "block_size = 800\n",
        "# batch_size = 128\n",
        "batch_size = 20\n",
        "num_embeddings = 384\n",
        "dropout = 0.2\n",
        "num_heads = 6\n",
        "num_layers = 6\n",
        "# percent_use = 0.5\n",
        "epochs = 32\n",
        "lr = 1e-3\n",
        "# vocab_size = tiktoken.get_encoding(\"r50k_base\").n_vocab\n",
        "vocab_size = 100265\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gwq9GXt81L3a",
        "outputId": "9017b0f1-35f1-4017-ea2e-1cf8fe40087c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lw69QZMse4Je",
        "outputId": "2ebe08f3-17bd-4094-f79a-83073a634dc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'root': '/content/drive/MyDrive/11685-hws/hw5/data',\n",
        "    'batch_size': batch_size,\n",
        "    'block_size': block_size,\n",
        "    'num_epochs': epochs\n",
        "}"
      ],
      "metadata": {
        "id": "1KpG1bSAkyJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "# pre-train data\n",
        "# train_data = np.memmap(os.path.join(config['root'], 'train.bin'), dtype=np.uint16, mode='r')\n",
        "# val_data = np.memmap(os.path.join(config['root'], 'val.bin'), dtype=np.uint16, mode='r')\n",
        "# # print(train_data.shape)"
      ],
      "metadata": {
        "id": "kWqXnJJkfDRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.manual_seed(1337)\n",
        "# def get_batch(split, batch_idx):\n",
        "#     data = train_data if split == 'train' else val_data\n",
        "#     idx = batch_idx * config['block_size'] * config['batch_size']\n",
        "#     # ix = torch.randint(len(data) - config['block_size'], (config['batch_size'],))\n",
        "#     # x = torch.from_numpy(data[idx:idx+config['block_size']].astype(np.int64))\n",
        "#     # y = torch.from_numpy(data[idx+1:idx+1+config['block_size']].astype(np.int64))\n",
        "#     x = torch.stack([torch.from_numpy((data[i:i+config['block_size']]).astype(np.int64)) for i in range(idx, idx + config['batch_size'] * config['block_size'], config['block_size'])])\n",
        "#     y = torch.stack([torch.from_numpy((data[i+1:i+1+config['block_size']]).astype(np.int64)) for i in range(idx, idx + config['batch_size'] * config['block_size'], config['block_size'])])\n",
        "#     x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "#     return x, y\n",
        "\n",
        "# print(\"Batch size: \", config['batch_size'])\n",
        "# train_data_len = len(train_data)\n",
        "# train_data_batches = train_data_len // (config['batch_size'] * config['block_size'])\n",
        "# train_data_batches = train_data_batches // 32\n",
        "# val_data_len = len(val_data)\n",
        "# val_data_batches = val_data_len // (config['batch_size'] * config['block_size'])\n",
        "# val_data_batches = val_data_batches // 8\n",
        "# print(\"Train dataset samples = {}, batches = {}\".format(train_data_len, train_data_batches))\n",
        "# print(\"Val dataset samples = {}, batches = {}\".format(val_data_len, val_data_batches))\n",
        "\n",
        "\n",
        "# xb, yb = get_batch('train', 0)\n",
        "# print('inputs:')\n",
        "# print(xb.shape)\n",
        "# print(xb)\n",
        "# print('targets:')\n",
        "# print(yb.shape)\n",
        "# print(yb)\n",
        "\n",
        "# print('----')\n",
        "\n",
        "# xb, yb = get_batch('train', 1)\n",
        "# print('inputs:')\n",
        "# print(xb.shape)\n",
        "# print(xb)\n",
        "# print('targets:')\n",
        "# print(yb.shape)\n",
        "# print(yb)"
      ],
      "metadata": {
        "id": "bo_ULNWGxmNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load SQuAD data\n",
        "import json\n",
        "\n",
        "def read_squad(path):\n",
        "    # open JSON file and load intro dictionary\n",
        "    with open(path, 'rb') as f:\n",
        "        squad_dict = json.load(f)\n",
        "\n",
        "    # initialize lists for contexts, questions, and answers\n",
        "    contexts = []\n",
        "    questions = []\n",
        "    answers = []\n",
        "    # iterate through all data in squad data\n",
        "    for group in squad_dict['data']:\n",
        "        for passage in group['paragraphs']:\n",
        "            context = passage['context']\n",
        "            for qa in passage['qas']:\n",
        "                question = qa['question']\n",
        "                # check if we need to be extracting from 'answers' or 'plausible_answers'\n",
        "                if 'plausible_answers' in qa.keys():\n",
        "                    access = 'plausible_answers'\n",
        "                else:\n",
        "                    access = 'answers'\n",
        "                for answer in qa[access]:\n",
        "                    # append data to lists\n",
        "                    contexts.append(context)\n",
        "                    questions.append(question)\n",
        "                    answers.append(answer)\n",
        "    # return formatted data lists\n",
        "    return contexts, questions, answers\n",
        "\n",
        "# execute our read SQuAD function for training and validation sets\n",
        "train_contexts, train_questions, train_answers = read_squad(os.path.join(config['root'], 'squad/train-v2.0.json'))\n",
        "val_contexts, val_questions, val_answers = read_squad(os.path.join(config['root'], 'squad/dev-v2.0.json'))"
      ],
      "metadata": {
        "id": "tUnmZUTj_rSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_contexts[0])\n",
        "print(train_questions[0])\n",
        "print(train_answers[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGR335PN_uar",
        "outputId": "f920913d-90dc-46eb-c00e-9a12e1491bbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
            "When did Beyonce start becoming popular?\n",
            "{'text': 'in the late 1990s', 'answer_start': 269}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_answers_text = [answer['text'] for answer in train_answers]\n",
        "val_answers_text = [answer['text'] for answer in val_answers]\n",
        "print(train_answers_text[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3bIglYH_yMC",
        "outputId": "2706861e-aa0e-4cf5-8789-164903acd01c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "in the late 1990s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load encoder\n",
        "cl100k_base = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "# In production, load the arguments directly instead of accessing private attributes\n",
        "# See openai_public.py for examples of arguments for specific encodings\n",
        "enc = tiktoken.Encoding(\n",
        "    # If you're changing the set of special tokens, make sure to use a different name\n",
        "    # It should be clear from the name what behaviour to expect.\n",
        "    name=\"cl100k_im\",\n",
        "    pat_str=cl100k_base._pat_str,\n",
        "    mergeable_ranks=cl100k_base._mergeable_ranks,\n",
        "    special_tokens={\n",
        "        **cl100k_base._special_tokens,\n",
        "        \"<|pad|>\": 100264,\n",
        "    }\n",
        ")\n",
        "pad_value = enc.encode(\"<|pad|>\", allowed_special=\"all\")[0]"
      ],
      "metadata": {
        "id": "1lLKwaDb_02S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer_prompt = enc.encode('\\nanswer: ')\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class SquadDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, context, question, answer, encoder, block_size):\n",
        "        self.block_size = block_size\n",
        "        # add 'context: ' to the beginning of each context so that our model knows where the context begins\n",
        "        context = ['context: ' + text for text in context]\n",
        "        # add 'question: ' to the beginning of each answer so that our model knows where the answer begins\n",
        "        question = ['question: ' + text for text in question]\n",
        "        # add 'answer: ' to the beginning of each answer so that our model knows where the answer begins\n",
        "        # answer = ['answer: ' + text for text in answer]\n",
        "        answer = [text for text in answer]\n",
        "\n",
        "        # concat our context and question into one string and separate them with a new line\n",
        "        # trucate our context/question to the maximum length the model can handle\n",
        "        # note: this is a special token used by GPT2 to indicate the end of text\n",
        "        self.X = []\n",
        "        for context, question in zip(context, question):\n",
        "            context = encoder.encode(context + ' \\n')\n",
        "            question = encoder.encode(question)\n",
        "            if len(context) + len(question) > block_size - len(answer_prompt):\n",
        "                context = context[:block_size - len(answer_prompt) - len(question)]\n",
        "            context.extend(question)\n",
        "            if len(context) > block_size - len(answer_prompt):\n",
        "                context = context[:block_size - len(answer_prompt)]\n",
        "            self.X.append(context)\n",
        "        self.Y = []\n",
        "        for text in answer:\n",
        "            text = encoder.encode(text)\n",
        "            if len(text) > block_size:\n",
        "                text = text[:block_size]\n",
        "            self.Y.append(text)\n",
        "        # self.X = [context + '\\n' + question + '\\nanswer: ' for context, question in zip(context, question)]\n",
        "        # tokenize our context, question and answer strings\n",
        "        # self.X = encoder.encode_batch(self.X)\n",
        "        # self.Y = encoder.encode_batch(answer)\n",
        "        # self.X = np.array(self.X)\n",
        "        # self.Y = np.array(self.Y)\n",
        "        assert len(self.X) == len(self.Y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.X[idx]\n",
        "        y = self.Y[idx]\n",
        "        return torch.tensor(x), torch.tensor(y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)//4\n",
        "\n",
        "    def collate_fn(self,batch):\n",
        "\n",
        "        batch_x, batch_y, lengths_x, lengths_y = [], [], [], []\n",
        "\n",
        "        for x, y in batch:\n",
        "            # Add the mfcc, transcripts and their lengths to the lists created above\n",
        "            # pad x and y with zeros to self.block_size\n",
        "            lengths_x.append(x.shape[0])\n",
        "            lengths_y.append(y.shape[0])\n",
        "            try:\n",
        "                x = np.pad(x, (0, self.block_size - len(answer_prompt) - len(x)), 'constant', constant_values=pad_value)\n",
        "                y = np.pad(y, (0, self.block_size - len(y)), 'constant', constant_values=pad_value)\n",
        "            except:\n",
        "                print(x.shape, y.shape)\n",
        "                print(x, y)\n",
        "                raise\n",
        "            x = np.concatenate((x, answer_prompt))\n",
        "            batch_x.append(x)\n",
        "            batch_y.append(y)\n",
        "\n",
        "        batch_x = np.array(batch_x)\n",
        "        batch_y = np.array(batch_y)\n",
        "        # pack the mfccs and transcripts using the pad_sequence function from pytorch\n",
        "        # batch_x_pad = torch.nn.utils.rnn.pad_sequence(batch_x, batch_first=True, padding_value=0)\n",
        "        # batch_y_pad = torch.nn.utils.rnn.pad_sequence(batch_y, batch_first=True, padding_value=0)\n",
        "\n",
        "        return torch.tensor(batch_x), torch.tensor(batch_y), torch.tensor(lengths_x), torch.tensor(lengths_y)\n",
        "\n",
        "# build datasets for both our training and validation sets\n",
        "train_dataset_squad = SquadDataset(train_contexts, train_questions, train_answers_text, enc, config['block_size'])\n",
        "val_dataset_squad = SquadDataset(val_contexts, val_questions, val_answers_text, enc, config['block_size'])"
      ],
      "metadata": {
        "id": "FHBxw---_3Ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load CNN Dailymail dataset\n",
        "dataset = load_dataset('cnn_dailymail', '3.0.0')"
      ],
      "metadata": {
        "id": "UwB5tvZq_7kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_article_total = [x['article'] for x in dataset['train']]\n",
        "train_highlights_total = [x['highlights'] for x in dataset['train']]\n",
        "val_article_total = [x['article'] for x in dataset['validation']]\n",
        "val_highlights_total = [x['highlights'] for x in dataset['validation']]"
      ],
      "metadata": {
        "id": "pScNT2RK__AR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 2\n",
        "fraction = 8\n",
        "train_article = train_article_total[idx*len(train_article_total)//fraction:(idx+1)*len(train_article_total)//fraction]\n",
        "train_highlights = train_highlights_total[idx*len(train_highlights_total)//fraction:(idx+1)*len(train_highlights_total)//fraction]\n",
        "val_article = val_article_total[:len(val_article_total)//8]\n",
        "val_highlights = val_highlights_total[:len(val_highlights_total)//8]"
      ],
      "metadata": {
        "id": "MOZ9LtYzDklh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_prompt = enc.encode('\\nSummary: ')\n",
        "class CNNDailyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, article, highlights, encoder, block_size):\n",
        "        self.block_size = block_size\n",
        "        self.X = []\n",
        "        for text in article:\n",
        "            text = 'Summarize this article: ' + text\n",
        "            text = encoder.encode(text[:block_size])\n",
        "            # if len(text) > block_size:\n",
        "            #     text = text[:block_size]\n",
        "            self.X.append(text)\n",
        "\n",
        "        self.Y = []\n",
        "        for text in highlights:\n",
        "            # text = 'Summary: ' + text\n",
        "            text = encoder.encode(text[:block_size])\n",
        "            # if len(text) > block_size:\n",
        "            #     text = text[:block_size]\n",
        "            self.Y.append(text)\n",
        "        assert len(self.X) == len(self.Y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.X[idx]\n",
        "        y = self.Y[idx]\n",
        "        return torch.tensor(x), torch.tensor(y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def collate_fn(self,batch):\n",
        "\n",
        "        batch_x, batch_y, lengths_x, lengths_y = [], [], [], []\n",
        "\n",
        "        for x, y in batch:\n",
        "            # Add the mfcc, transcripts and their lengths to the lists created above\n",
        "            # pad x and y with zeros to self.block_size\n",
        "            lengths_x.append(x.shape[0])\n",
        "            lengths_y.append(y.shape[0])\n",
        "            x = np.pad(x, (0, self.block_size - len(summary_prompt) - len(x)), 'constant', constant_values=pad_value)\n",
        "            y = np.pad(y, (0, self.block_size - len(y)), 'constant', constant_values=pad_value)\n",
        "            x = np.concatenate((x, summary_prompt))\n",
        "            # y = np.concatenate((y, answer_prompt))\n",
        "            batch_x.append(x)\n",
        "            batch_y.append(y)\n",
        "\n",
        "        batch_x = np.array(batch_x)\n",
        "        batch_y = np.array(batch_y)\n",
        "        # pack the mfccs and transcripts using the pad_sequence function from pytorch\n",
        "        # batch_x_pad = torch.nn.utils.rnn.pad_sequence(batch_x, batch_first=True, padding_value=0)\n",
        "        # batch_y_pad = torch.nn.utils.rnn.pad_sequence(batch_y, batch_first=True, padding_value=0)\n",
        "\n",
        "        return torch.tensor(batch_x), torch.tensor(batch_y), torch.tensor(lengths_x), torch.tensor(lengths_y)\n",
        "\n",
        "# build datasets for both our training and validation sets\n",
        "train_dataset_cnn = CNNDailyDataset(train_article, train_highlights, enc, config['block_size'])\n",
        "val_dataset_cnn = CNNDailyDataset(val_article, val_highlights, enc, config['block_size'])"
      ],
      "metadata": {
        "id": "JK1UnRLvAOxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader_cnn    = torch.utils.data.DataLoader(\n",
        "    dataset     = train_dataset_cnn,\n",
        "    batch_size  = config['batch_size'],\n",
        "    shuffle     = True,\n",
        "    num_workers = 4,\n",
        "    pin_memory  = True,\n",
        "    drop_last   = True,\n",
        "    collate_fn  = train_dataset_cnn.collate_fn\n",
        ")\n",
        "\n",
        "valid_loader_cnn    = torch.utils.data.DataLoader(\n",
        "    dataset     = val_dataset_cnn,\n",
        "    batch_size  = config['batch_size'],\n",
        "    shuffle     = False,\n",
        "    num_workers = 2,\n",
        "    pin_memory  = True,\n",
        "    drop_last   = True,\n",
        "    collate_fn  = val_dataset_cnn.collate_fn\n",
        ")\n",
        "\n",
        "print(\"No. of train mfccs   : \", train_dataset_cnn.__len__())\n",
        "print(\"Batch size           : \", config['batch_size'])\n",
        "print(\"Train batches        : \", train_loader_cnn.__len__())\n",
        "print(\"Valid batches        : \", valid_loader_cnn.__len__())\n",
        "\n",
        "print(\"\\nChecking the shapes of the data...\")\n",
        "for batch in train_loader_cnn:\n",
        "    x, y, x_len, y_len = batch\n",
        "    print(x.shape, y.shape, x_len.shape, y_len.shape)\n",
        "    print(x[0])\n",
        "    print(enc.decode(x[0].tolist()))\n",
        "    print(y[0])\n",
        "    print(enc.decode(y[0].tolist()))\n",
        "    print(y_len[0])\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByvX8vucApZ5",
        "outputId": "31a14ce6-eb0c-4c18-90b9-50a1c2c6b66d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of train mfccs   :  35889\n",
            "Batch size           :  20\n",
            "Train batches        :  1794\n",
            "Valid batches        :  83\n",
            "\n",
            "Checking the shapes of the data...\n",
            "torch.Size([20, 800]) torch.Size([20, 800]) torch.Size([20]) torch.Size([20])\n",
            "tensor([  9370,   5730,    553,    420,   4652,     25,   3296,    662,   7957,\n",
            "         58344,     11,   5492,  23298,  12865,    662,    393,  14451,  39979,\n",
            "            25,    662,    220,    777,     25,   2371,  26963,     11,    220,\n",
            "          1627,   6841,    220,    679,     17,    662,    765,    662,  78961,\n",
            "            25,    662,    220,   2437,     25,   1987,  26963,     11,    220,\n",
            "          1544,   6841,    220,    679,     17,    662,    326,   6564,  20550,\n",
            "          2011,    387,   2728,   1949,   6514,    555,    480,  21051,     11,\n",
            "          1234,    264,  49737,  20733,   1608,    858,  23415,   1523,    555,\n",
            "         37381,  20258,     13,    578,  16410,    690,    387,   7318,   7083,\n",
            "          1227,    264,   1060,    369,   1855,  52216,    814,   2231,    389,\n",
            "           872,   6603,   1389,    279,   1890,   3392,    814,   5371,    369,\n",
            "           682,    502,   6978,     13,    578,  19351,     11,    902,    574,\n",
            "         43578,    704,    304,   5887,     11,   5415,    430,    459,  25355,\n",
            "         20792,    649,   4254,    449,    264,  10896,   3984,    814,    527,\n",
            "           304,    279,   3158,    369,    810,   1109,    220,   1187,   4207,\n",
            "            13,    362,  24618,    753,  18101,   4250,    387,  77103,    389,\n",
            "           279,  21319,    430,    279,   8893,   1587,    539,    617,   4443,\n",
            "         22654,    477,  11311,    315,  22423,     13,  82661,     25,    480,\n",
            "         21051,    617,   1027,   3309,    814,   2011,   3041,  12079,  20550,\n",
            "          1949,   6514,    320,   1213,   6945,      8,    578,   7754,    264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264,    198,  19791,     25,    220])\n",
            "Summarize this article: By . James Slack, Home Affairs Editor . PUBLISHED: . 19:04 EST, 26 November 2012 . | . UPDATED: . 02:38 EST, 27 November 2012 . llegal immigrants must be given free treatment by GPs, under a hugely controversial edict handed down by NHS managers. The doctors will be paid £64 a year for each migrant they put on their books – the same amount they receive for all new patients. The guidance, which was slipped out in July, states that an overseas visitor can register with a doctor provided they are in the area for more than 24 hours. A GP’s appointment cannot be withheld on the grounds that the patient does not have personal identification or proof of residence. Guidance: GPs have been told they must give illegal immigrants free treatment (file picture) The instruction a<|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|>\n",
            "Summary: \n",
            "tensor([  3648,  19351,   2795,    480,  21051,   4250,  87900,  18101,    389,\n",
            "           279,  21319,    430,   8893,   1587,    539,    617,  22654,  16853,\n",
            "          5901,     82,   9454,   8771,    323,  40796,   2100,    986,   1071,\n",
            "           279,   6671,    374,    364,   3518,   9887,  44085,      6, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264])\n",
            "New guidance says GPs cannot withhold appointment on the grounds that patient does not have identification .\n",
            "MPs Frank Field and Nicholas Soames said the situation is 'absolutely unacceptable'<|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|>\n",
            "tensor(35)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader_squad    = torch.utils.data.DataLoader(\n",
        "    dataset     = train_dataset_squad,\n",
        "    batch_size  = config['batch_size'],\n",
        "    shuffle     = True,\n",
        "    num_workers = 4,\n",
        "    pin_memory  = True,\n",
        "    drop_last   = True,\n",
        "    collate_fn  = train_dataset_squad.collate_fn\n",
        ")\n",
        "\n",
        "valid_loader_squad    = torch.utils.data.DataLoader(\n",
        "    dataset     = val_dataset_squad,\n",
        "    batch_size  = config['batch_size'],\n",
        "    shuffle     = False,\n",
        "    num_workers = 2,\n",
        "    pin_memory  = True,\n",
        "    drop_last   = True,\n",
        "    collate_fn  = val_dataset_squad.collate_fn\n",
        ")\n",
        "\n",
        "print(\"No. of train mfccs   : \", train_dataset_squad.__len__())\n",
        "print(\"Batch size           : \", config['batch_size'])\n",
        "print(\"Train batches        : \", train_loader_squad.__len__())\n",
        "print(\"Valid batches        : \", valid_loader_squad.__len__())\n",
        "\n",
        "print(\"\\nChecking the shapes of the data...\")\n",
        "for batch in train_loader_squad:\n",
        "    x, y, x_len, y_len = batch\n",
        "    print(x.shape, y.shape, x_len.shape, y_len.shape)\n",
        "    print(x[0])\n",
        "    print(enc.decode(x[0].tolist()))\n",
        "    print(y[0])\n",
        "    print(enc.decode(y[0].tolist()))\n",
        "    print(y_len[0])\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgPYGDWGAtg0",
        "outputId": "afb0babe-dd49-4b38-bd6b-ddc792676e27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of train mfccs   :  32579\n",
            "Batch size           :  20\n",
            "Train batches        :  1628\n",
            "Valid batches        :  327\n",
            "\n",
            "Checking the shapes of the data...\n",
            "torch.Size([20, 800]) torch.Size([20, 800]) torch.Size([20]) torch.Size([20])\n",
            "tensor([  2196,     25,  35132,   6072,    369,  22735,   5045,    527,  20816,\n",
            "           304,   3116,  21562,     25,    469,     16,     11,    469,     17,\n",
            "            11,    469,     20,     33,     11,    323,    469,     21,    323,\n",
            "         28347,    449,  10845,  85234,     13,    578,   2144,    315,  50917,\n",
            "          1436,    387,  17125,    505,    279,   1486,    315,   1684,    315,\n",
            "           279,  14291,   2955,     11,    719,    389,    279,   1023,   1450,\n",
            "         25930,    279,   4819,    315,    958,  37748,  32317,     11,   5423,\n",
            "          2949,    469,     16,    323,    469,     17,  21562,     11,    902,\n",
            "           527,  20816,    369,  10845,  85234,    596,  17880,  35319,   2532,\n",
            "            13,   4452,     11,   1234,   7327,  13875,  51271,   9323,    320,\n",
            "         49399,      8,  10396,     11,    279,   1176,   7140,    311,   1212,\n",
            "         49927,    304,    264,   3230,  11900,    690,    617,  10844,    311,\n",
            "           430,  11900,     11,    323,    904,  17876,   3932,    690,    387,\n",
            "          2631,    311,   6994,   8041,   4972,    311,   1701,    430,  11900,\n",
            "            11,    323,   6062,   6106,    430,    872,  66198,    656,    539,\n",
            "         40978,    449,    279,   4113,   7140,    596,  66198,     13,   1102,\n",
            "          1457,   8111,    430,   8620,  22735,   5045,  47710,    690,   1212,\n",
            "         78768,    304,    279,    469,     16,     11,    469,     17,     11,\n",
            "           469,     20,     33,     11,    323,    469,     21,  21562,   1603,\n",
            "          4606,    596,  10845,  85234,  47710,    323,   8617,    617,   6156,\n",
            "          3268,    311,   1521,  11900,  21986,     13,    720,   7998,     25,\n",
            "         16299,  11900,  21562,    527,   1455,   4461,    311,   5353,   4819,\n",
            "           315,    958,  37748,  32317,    449,    279,  10845,  85234,   1887,\n",
            "            30, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264,    198,   9399,     25,    220])\n",
            "context: Frequencies for COMPASS are allocated in four bands: E1, E2, E5B, and E6 and overlap with Galileo. The fact of overlapping could be convenient from the point of view of the receiver design, but on the other hand raises the issues of inter-system interference, especially within E1 and E2 bands, which are allocated for Galileo's publicly regulated service. However, under International Telecommunication Union (ITU) policies, the first nation to start broadcasting in a specific frequency will have priority to that frequency, and any subsequent users will be required to obtain permission prior to using that frequency, and otherwise ensure that their broadcasts do not interfere with the original nation's broadcasts. It now appears that Chinese COMPASS satellites will start transmitting in the E1, E2, E5B, and E6 bands before Europe's Galileo satellites and thus have primary rights to these frequency ranges. \n",
            "question: Which frequency bands are most likely to cause issues of inter-system interference with the Galileo system?<|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|>\n",
            "answer: \n",
            "tensor([    36,     16,    323,    469,     17, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264,\n",
            "        100264, 100264, 100264, 100264, 100264, 100264, 100264, 100264])\n",
            "E1 and E2<|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|>\n",
            "tensor(5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "class self_attention(nn.Module):\n",
        "    def __init__(self, head_size, num_embeddings):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(num_embeddings, head_size, bias=False)\n",
        "        self.key = nn.Linear(num_embeddings, head_size, bias=False)\n",
        "        self.value = nn.Linear(num_embeddings, head_size, bias=False)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.Q = self.query(x)\n",
        "        self.K = self.key(x)\n",
        "        self.V = self.value(x)\n",
        "\n",
        "        t = self.Q.size(dim=1)\n",
        "        lower_t = torch.tril(torch.ones(t, t)).to(device)\n",
        "        self.A_w = torch.matmul(self.Q, torch.permute(self.K, (0, 2, 1)))\n",
        "        d_k_root = torch.sqrt(torch.tensor(self.K.size(dim=2)))\n",
        "        self.A_w = self.A_w.masked_fill(lower_t == 0, float('-inf'))\n",
        "        self.A_sig = self.softmax(torch.div(self.A_w, d_k_root))\n",
        "        self.A_sig = self.dropout(self.A_sig)\n",
        "        X_new = torch.bmm(self.A_sig, self.V)\n",
        "\n",
        "        return X_new\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, num_heads, head_size, num_embeddings):\n",
        "        super(MultiheadAttention, self).__init__()\n",
        "        self.multihead = nn.ModuleList([self_attention(head_size, num_embeddings) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(num_embeddings, num_embeddings)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([head(x) for head in self.multihead], dim=-1)\n",
        "        return self.dropout(self.proj(out))\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.block = nn.Sequential(nn.Linear(num_embeddings, 4 * num_embeddings), nn.GELU(),\n",
        "                                   nn.Linear(4 * num_embeddings, num_embeddings), nn.Dropout(dropout))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = MultiheadAttention(num_heads=num_heads, head_size=num_embeddings // num_heads,\n",
        "                                            num_embeddings=num_embeddings)\n",
        "        self.feed_forward = FeedForward()\n",
        "        self.ln1, self.ln2 = nn.LayerNorm(num_embeddings), nn.LayerNorm(num_embeddings)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attention(self.ln1(x))\n",
        "        x = x + self.feed_forward(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.tok_embedding = nn.Embedding(vocab_size, num_embeddings)\n",
        "        self.pos_embedding = nn.Embedding(block_size, num_embeddings)\n",
        "        self.transformer_blocks = nn.Sequential(*[TransformerBlock() for _ in range(num_layers)])\n",
        "        self.final_ln = nn.LayerNorm(num_embeddings)\n",
        "        self.final_linear = nn.Linear(num_embeddings, vocab_size)\n",
        "        self.B = 0\n",
        "        self.T = 0\n",
        "        self.C = 0\n",
        "        # self.tok_embedding.weight = self.final_linear.weight\n",
        "    def forward(self, x):\n",
        "        tok_embed = self.tok_embedding(x)\n",
        "        pos_embed = self.pos_embedding(torch.arange(block_size, device=device))\n",
        "        x = tok_embed + pos_embed\n",
        "        x = self.transformer_blocks(x)\n",
        "        x = self.final_ln(x)\n",
        "        logits = self.final_linear(x)\n",
        "        B, T, C = logits.shape\n",
        "        self.B = B\n",
        "        self.T = T\n",
        "        self.C = C\n",
        "        logits = logits.view(B * T, C)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        _,T = idx.shape\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits = self(idx_cond)\n",
        "            logits = logits.view(self.B, self.T, self.C)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :]  # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = nn.functional.softmax(logits, dim=-1)  # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
        "        return idx[:,T:]"
      ],
      "metadata": {
        "id": "FKOg9mt61o1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "model = Model().to(device)\n",
        "# summary(model, xb.to(device))\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2)\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ],
      "metadata": {
        "id": "SvfYhMKF1o9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-train\n",
        "# from tqdm import tqdm\n",
        "\n",
        "# def train_model(model, criterion, optimizer, start):\n",
        "\n",
        "#     model.train()\n",
        "#     batch_bar = tqdm(total=train_data_batches, dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
        "\n",
        "#     total_loss = 0\n",
        "#     minibatch = 0\n",
        "#     for k in range(start, start + train_data_batches):\n",
        "#         # print(k)\n",
        "#         # try:\n",
        "#         X, Y = get_batch('train', k)\n",
        "#         with torch.cuda.amp.autocast():\n",
        "#             logits = model(X)\n",
        "#             loss = criterion(logits.view(-1, logits.size(-1)), Y.view(-1))\n",
        "#         total_loss += loss.item()\n",
        "#         batch_bar.set_postfix(\n",
        "#             loss=\"{:.04f}\".format(float(total_loss / (k + 1))),\n",
        "#             lr=\"{:.06f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "#         # Add this check during training loop\n",
        "#         if math.isnan(total_loss):\n",
        "#             print(\"NaN loss encountered. Stopping training.\")\n",
        "#             print(X)\n",
        "#             print(Y)\n",
        "#             print(logits)\n",
        "#             print(loss)\n",
        "#             break\n",
        "#         batch_bar.update() # Update tqdm bar\n",
        "#         # Another couple things you need for FP16.\n",
        "#         scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
        "#         scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
        "#         scaler.update() # This is something added just for FP16\n",
        "\n",
        "#         del X, Y, loss\n",
        "#         torch.cuda.empty_cache()\n",
        "#         gc.collect()\n",
        "#         # print(\"minibatch finished: \", minibatch)\n",
        "#         minibatch += 1\n",
        "#         # except:\n",
        "#         #     print(f'{k} batch dropped')\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "\n",
        "#     batch_bar.close() # You need this to close the tqdm bar\n",
        "\n",
        "#     return total_loss / train_data_batches, k\n",
        "\n",
        "\n",
        "# def validate_model(model, criterion):\n",
        "\n",
        "#     model.eval()\n",
        "#     batch_bar = tqdm(total=val_data_batches, dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
        "\n",
        "#     total_loss = 0\n",
        "\n",
        "#     for k in range(val_data_batches):\n",
        "\n",
        "#         # try:\n",
        "#         X, Y = get_batch('val', k)\n",
        "#         with torch.inference_mode():\n",
        "#             logits = model(X)\n",
        "#             loss = criterion(logits.view(-1, logits.size(-1)), Y.view(-1))\n",
        "\n",
        "#         total_loss += float(loss)\n",
        "\n",
        "#         batch_bar.set_postfix(loss=\"{:.04f}\".format(float(total_loss / (k + 1))))\n",
        "\n",
        "#         batch_bar.update()\n",
        "\n",
        "#         del X, Y, loss\n",
        "#         torch.cuda.empty_cache()\n",
        "#         gc.collect()\n",
        "#         # except:\n",
        "#         #     print(f'{k} batch dropped')\n",
        "\n",
        "\n",
        "\n",
        "#     batch_bar.close()\n",
        "#     total_loss = total_loss/val_data_batches\n",
        "\n",
        "#     return total_loss"
      ],
      "metadata": {
        "id": "r1caFxfBy_KD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, optimizer, scheduler, loss, epoch, path):\n",
        "    torch.save(\n",
        "        {'model_state_dict'         : model.state_dict(),\n",
        "         'optimizer_state_dict'     : optimizer.state_dict(),\n",
        "         'scheduler_state_dict'     : scheduler.state_dict(),\n",
        "         'epoch'                    : epoch,\n",
        "         'loss'                     : loss,\n",
        "         },\n",
        "         path\n",
        "    )\n",
        "\n",
        "def load_model(path, model, optimizer= None, scheduler= None):\n",
        "\n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    if optimizer != None:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    if scheduler != None:\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "\n",
        "    epoch   = checkpoint['epoch']\n",
        "    loss  = checkpoint['loss']\n",
        "\n",
        "    return [model, optimizer, scheduler, epoch, loss]"
      ],
      "metadata": {
        "id": "iIiff6tj17_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def itr_merge(*itrs):\n",
        "    for itr in itrs:\n",
        "        for v in itr:\n",
        "            yield v"
      ],
      "metadata": {
        "id": "DdnrS1_eBXx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tune\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train_model(model, criterion, optimizer):\n",
        "\n",
        "    model.train()\n",
        "    batch_bar = tqdm(total=len(train_loader_squad)+len(train_loader_cnn), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
        "\n",
        "    total_loss = 0\n",
        "    minibatch = 0\n",
        "    # for i, data in enumerate(train_loader):\n",
        "    for i, data in enumerate(itr_merge(train_loader_cnn, train_loader_squad)):\n",
        "        # print(k)\n",
        "        # try:\n",
        "        X, Y = data[0].to(device), data[1].to(device)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            logits = model(X)\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), Y.view(-1))\n",
        "        total_loss += loss.item()\n",
        "        batch_bar.set_postfix(\n",
        "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "            lr=\"{:.06f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "        # Add this check during training loop\n",
        "        if math.isnan(total_loss):\n",
        "            print(\"NaN loss encountered. Stopping training.\")\n",
        "            print(X)\n",
        "            print(Y)\n",
        "            print(logits)\n",
        "            print(loss)\n",
        "            break\n",
        "        batch_bar.update() # Update tqdm bar\n",
        "        # Another couple things you need for FP16.\n",
        "        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
        "        scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
        "        scaler.update() # This is something added just for FP16\n",
        "\n",
        "        del X, Y, loss\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        # print(\"minibatch finished: \", minibatch)\n",
        "        minibatch += 1\n",
        "        # except:\n",
        "        #     print(f'{k} batch dropped')\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "\n",
        "    batch_bar.close() # You need this to close the tqdm bar\n",
        "\n",
        "    return total_loss / (len(train_loader_squad)+len(train_loader_cnn))\n",
        "\n",
        "\n",
        "def validate_model(model, criterion):\n",
        "\n",
        "    model.eval()\n",
        "    batch_bar = tqdm(total=len(valid_loader_squad)+len(valid_loader_cnn), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    # for i, data in enumerate(valid_loader):\n",
        "    for i, data in enumerate(itr_merge(valid_loader_cnn, valid_loader_squad)):\n",
        "\n",
        "        # try:\n",
        "        X, Y = data[0].to(device), data[1].to(device)\n",
        "        with torch.inference_mode():\n",
        "            logits = model(X)\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), Y.view(-1))\n",
        "\n",
        "        total_loss += float(loss)\n",
        "\n",
        "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(total_loss / (i + 1))))\n",
        "\n",
        "        batch_bar.update()\n",
        "\n",
        "        del X, Y, loss\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        # except:\n",
        "        #     print(f'{k} batch dropped')\n",
        "\n",
        "\n",
        "\n",
        "    batch_bar.close()\n",
        "    total_loss = total_loss/(len(valid_loader_squad)+len(valid_loader_cnn))\n",
        "\n",
        "    return total_loss"
      ],
      "metadata": {
        "id": "YjY979jXBZW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login(key=\"87d4ded850f739673d588bb48fc800a2590c2f12\")\n",
        "run = wandb.init(\n",
        "    # name=\"LLM-Training-100k\",  ## Wandb creates random run names if you skip this field\n",
        "    name=\"LLM-Training-100k-800-finetune\",  ## Wandb creates random run names if you skip this field\n",
        "    # reinit=True,  ### Allows reinitalizing runs when you re-run this cell\n",
        "    id = '5rphxddv',### Insert specific run id here if you want to resume a previous run\n",
        "    resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "    project=\"hw5-ablations\",  ### Project should be created in your wandb account\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "a2MA6lkk2LWh",
        "outputId": "119ce0d7-fcd8-47cf-fe6c-f2b441cca26f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjding3\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20231213_015831-5rphxddv</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Resuming run <strong><a href='https://wandb.ai/jding3/hw5-ablations/runs/5rphxddv' target=\"_blank\">LLM-Training-100k-800-finetune</a></strong> to <a href='https://wandb.ai/jding3/hw5-ablations' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jding3/hw5-ablations' target=\"_blank\">https://wandb.ai/jding3/hw5-ablations</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jding3/hw5-ablations/runs/5rphxddv' target=\"_blank\">https://wandb.ai/jding3/hw5-ablations/runs/5rphxddv</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_path = '/content/drive/MyDrive/11685-hws/hw5/model/best_checkpoint-800-finetune.pth'\n",
        "model, optimizer, scheduler, start, best_loss = load_model(best_model_path, model, optimizer, scheduler)\n",
        "start += 1\n",
        "print(start)\n",
        "print(best_loss)"
      ],
      "metadata": {
        "id": "EAiQ6hQl3Ffv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab58b154-8be5-4517-a3bb-c0d285667ea5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "8.642220121476708\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_model_path = '/content/drive/MyDrive/11685-hws/hw5/model/checkpoint-800-finetune.pth'\n",
        "model, optimizer, scheduler, start, best_loss = load_model(epoch_model_path, model, optimizer, scheduler)\n",
        "start += 1\n",
        "print(start)\n",
        "print(best_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkpWClt04nPt",
        "outputId": "e2a90ab2-f0e7-4464-e564-7149be143ddb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "8.75435784153822\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "# compute bleu\n",
        "for i, batch in enumerate(valid_loader_cnn):\n",
        "    x, y, x_len, y_len = batch\n",
        "    ave_len = torch.mean(y_len.float())\n",
        "    predictions = model.generate(x.to(device), int(ave_len))\n",
        "    predictions = enc.decode_batch(predictions.tolist())\n",
        "    references = [enc.decode(y[i].tolist()) for i in range(y.shape[0])]\n",
        "    references = [reference.replace('<|pad|>', '') for reference in references]\n",
        "    references = [reference.split() for reference in references]\n",
        "    for prediction in predictions:\n",
        "        prediction = prediction.split()\n",
        "        score += sentence_bleu(references, prediction)\n",
        "    if i == 10:\n",
        "        break\n",
        "    del x, y, x_len, y_len, predictions, references\n",
        "print(score/40)"
      ],
      "metadata": {
        "id": "S0PDJ5CJFTIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute rouge\n",
        "import evaluate\n",
        "rouge = evaluate.load('rouge')\n",
        "predictions = []\n",
        "references = []\n",
        "score = 0\n",
        "for i, batch in enumerate(valid_loader_squad):\n",
        "    x, y, x_len, y_len = batch\n",
        "    ave_len = torch.mean(y_len.float())\n",
        "    predictions = model.generate(x.to(device), int(ave_len))\n",
        "    predictions = enc.decode_batch(predictions.tolist())\n",
        "    references = [enc.decode(y[i].tolist()) for i in range(y.shape[0])]\n",
        "    predictions.extend(predictions)\n",
        "    references.extend(references)\n",
        "    del x, y, x_len, y_len\n",
        "    if i == 10:\n",
        "        break\n",
        "score = rouge.compute(predictions=predictions, references=references)\n",
        "print(score)"
      ],
      "metadata": {
        "id": "0EDH1VSKFQXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# start = 0\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=100264)\n",
        "# start_batch = 2090 * start\n",
        "# start_batch = 0\n",
        "# best_loss = float('inf')\n",
        "best_model_path = '/content/drive/MyDrive/11685-hws/hw5/model/best_checkpoint-800-finetune.pth'\n",
        "epoch_model_path = '/content/drive/MyDrive/11685-hws/hw5/model/checkpoint-800-finetune.pth'\n",
        "\n",
        "for epoch in range(start, config['num_epochs']):\n",
        "    # gc.collect()\n",
        "    # torch.cuda.empty_cache()\n",
        "    # start_batch = epoch * 8163\n",
        "    print(\"\\nEpoch: {}/{}\".format(epoch+1, config['num_epochs']))\n",
        "\n",
        "    # Call train and validate, get attention weights from training\n",
        "    optimizer.param_groups[0]['lr'] /= 10\n",
        "    curr_lr = optimizer.param_groups[0]['lr']\n",
        "    # print(start_batch)\n",
        "    # train_loss, end_batch = train_model(model, criterion, optimizer, int(start_batch))\n",
        "    # if epoch % 2 == 0:\n",
        "    # start_batch = end_batch - train_data_batches // 2 + 1\n",
        "    # valid_loss = validate_model(model, criterion)\n",
        "    train_loss = train_model(model, criterion, optimizer)\n",
        "    valid_loss = validate_model(model, criterion)\n",
        "    # Print your metrics\n",
        "    print(\"\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_loss, curr_lr))\n",
        "    print(\"\\tVal Loss {:.04f}%\\t\".format(valid_loss))\n",
        "    # Log metrics to Wandb\n",
        "    wandb.log({\n",
        "        'train_loss': train_loss,\n",
        "        'valid_loss': valid_loss,\n",
        "        'lr'        : curr_lr,\n",
        "    })\n",
        "\n",
        "    # Optional: Scheduler Step / Teacher Force Schedule Step\n",
        "    scheduler.step(valid_loss)\n",
        "    # tf_rate = max(tf_rate - 0.05, 0.0)\n",
        "\n",
        "    save_model(model, optimizer, scheduler, valid_loss, epoch, epoch_model_path)\n",
        "    wandb.save(epoch_model_path)\n",
        "    print(\"Saved epoch model\")\n",
        "\n",
        "    if valid_loss <= best_loss:\n",
        "        best_loss = valid_loss\n",
        "        save_model(model, optimizer, scheduler, valid_loss, epoch, best_model_path)\n",
        "        wandb.save(best_model_path)\n",
        "        print(\"Saved best model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hX39LUetzPTU",
        "outputId": "8eaa0941-9d62-4bab-e1a9-89dcaa3fcd8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 3/32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train:  50%|█████     | 1716/3422 [27:43<27:11,  1.05it/s, loss=7.7393, lr=0.000010]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# best_model_path = '/content/drive/MyDrive/11685-hws/hw5/model/best_checkpoint-800.pth'\n",
        "# save_model(model, optimizer, scheduler, valid_loss, epoch, best_model_path)"
      ],
      "metadata": {
        "id": "oIcw9V7d-Mnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "model, optimizer, scheduler, start, best_loss = load_model(epoch_model_path, model, optimizer, scheduler)\n",
        "score = 0\n",
        "# compute bleu\n",
        "for i, batch in enumerate(valid_loader_cnn):\n",
        "    x, y, x_len, y_len = batch\n",
        "    ave_len = torch.mean(y_len.float())\n",
        "    predictions = model.generate(x, ave_len)\n",
        "    predictions = enc.decode_batch(predictions.tolist())\n",
        "    references = [enc.decode(y[i].tolist()) for i in range(y.shape[0])]\n",
        "    references = [reference.replace('<|pad|>', '') for reference in references]\n",
        "    references = [reference.split() for reference in references]\n",
        "    for prediction in predictions:\n",
        "        prediction = prediction.split()\n",
        "        score += sentence_bleu(reference, candidate)\n",
        "    if i == 10:\n",
        "        break\n",
        "print(score/40)"
      ],
      "metadata": {
        "id": "x2AvZ5gi7P-L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}